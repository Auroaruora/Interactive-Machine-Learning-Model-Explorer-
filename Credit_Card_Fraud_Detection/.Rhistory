library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(recipes)
library(rpart)
library(rsample)
library(randomForest)
library(xgboost)
library(tidymodels)
library(tidyverse)
data <- read.table(file.choose(), header = FALSE, sep = ",")
str(data)
sapply(select(data,V1, V4, V5, V6, V7, V9, V10, V12, V13, V16), unique)
data$V2 <- as.numeric(data$V2)
data$V14 <- as.numeric(data$V14)
data <- data %>%
mutate(across(c(V1, V4, V5, V6, V7, V9, V10, V12, V13, V16), ~na_if(.x, "?")))
data$V16 <- factor(data$V16, levels = c("+", "-"),
labels = c("Positive", "Negative"))
blueprint <- recipe(V16 ~ ., data = data) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_impute_knn(all_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_dummy(all_nominal(), -all_outcomes())
blueprint_prep <- prep(blueprint, training = data)
set.seed(1)
index <- createDataPartition(data$V16, p = 0.7, list = FALSE)
data_train <- data[index, ]
data_test <- data[-index, ]
transformed_train <- bake(blueprint_prep, new_data = data_train)
transformed_test <- bake(blueprint_prep, new_data = data_test)
# Visualization
# Calculate the percentage
train_distribution <- data_train %>%
count(V16) %>%
mutate(Percentage = n / sum(n) * 100)
test_distribution <- data_test %>%
count(V16) %>%
mutate(Percentage = n / sum(n) * 100)
# Combine test and train set
combined_distribution <- bind_rows(
mutate(train_distribution, Set = "Train"),
mutate(test_distribution, Set = "Test")
)
# Plotting
ggplot(combined_distribution, aes(x = V16, y = Percentage, fill = Set)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "Target Feature", y = "Percentage", title = "Distribution of Target Feature in Training and Testing Sets") +
theme_minimal()
initial_rf <- randomForest(V16 ~ ., data = transformed_train)
plot(initial_rf)
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(mtry = c(13, 15, 17, 20, 22, 25),
splitrule = c("gini", "extratrees"),
min.node.size = c(5, 7, 9))
rf_fit <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
verbose = FALSE,
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC",
num.trees = 300)
## Plotting Results
ggplot(rf_fit)
ggplot(rf_fit, metric = "Sens")   # Sensitivity
#
trellis.par.set(caretTheme()) # optional
plot(rf_fit, metric = "ROC", plotType = "level")
# Final Model
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
RF_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(mtry = rf_fit$bestTune$mtry,
min.node.size = rf_fit$bestTune$min.node.size,
splitrule = rf_fit$bestTune$splitrule),
num.trees = 300)
# Training Set Results
RF_pred_train <- predict(RF_final, newdata = transformed_train)
RF_train_results <- confusionMatrix(transformed_train$V16, RF_pred_train)
print(RF_train_results)
# Test Set Results
RF_pred_test <- predict(RF_final, newdata = transformed_train)
RF_test_results <- confusionMatrix(transformed_train$V16, RF_pred_test)
print(RF_test_results)
set.seed(1)
#xg_spec <- boost_tree(
#    trees = 300,
#    tree_depth = tune(),
#    min_n = tune(),
#    mtry = tune(),
#    learn_rate = tune(),
#    loss_reduction = 0, # Not tuning this in this example
#    sample_size = 0.7  # Fixed in this example
#  ) %>%
#  set_mode("classification") %>%
#  set_engine("xgboost")
# Setup for cross-validation
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = data_train_processed,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
set.seed(1)
#xg_spec <- boost_tree(
#    trees = 300,
#    tree_depth = tune(),
#    min_n = tune(),
#    mtry = tune(),
#    learn_rate = tune(),
#    loss_reduction = 0, # Not tuning this in this example
#    sample_size = 0.7  # Fixed in this example
#  ) %>%
#  set_mode("classification") %>%
#  set_engine("xgboost")
# Setup for cross-validation
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = transformed_train,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
## Plotting Results
ggplot(xg_fit)
ggplot(xg_fit, metric = "Sens")   # Sensitivity
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
plot(xg_fit, metric = "ROC", plotType = "level")
plot(xg_fit, metric = "ROC", plotType = "level")
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
xg_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(mtry = xg_fit$bestTune$mtry,
min.node.size = xg_fit$bestTune$min.node.size,
splitrule = xg_fit$bestTune$splitrule),
num.trees = 300)
set.seed(1)
#xg_spec <- boost_tree(
#    trees = 300,
#    tree_depth = tune(),
#    min_n = tune(),
#    mtry = tune(),
#    learn_rate = tune(),
#    loss_reduction = 0, # Not tuning this in this example
#    sample_size = 0.7  # Fixed in this example
#  ) %>%
#  set_mode("classification") %>%
#  set_engine("xgboost")
# Setup for cross-validation
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
#max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = transformed_train,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
set.seed(1)
#xg_spec <- boost_tree(
#    trees = 300,
#    tree_depth = tune(),
#    min_n = tune(),
#    mtry = tune(),
#    learn_rate = tune(),
#    loss_reduction = 0, # Not tuning this in this example
#    sample_size = 0.7  # Fixed in this example
#  ) %>%
#  set_mode("classification") %>%
#  set_engine("xgboost")
# Setup for cross-validation
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = transformed_train,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
## Plotting Results
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
# Final Model
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
xg_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(max_depth = xg_fit$bestTune$max_depth,
eta = xg_fit$bestTune$eta,
min_child_weight = xg_fit$bestTune$min_child_weight,
subsample =xg_fit$bestTune$subsample
),
num.trees = 300)
RF_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(mtry = rf_fit$bestTune$mtry,
min.node.size = rf_fit$bestTune$min.node.size,
splitrule = rf_fit$bestTune$splitrule),
num.trees = 300)
# Training Set Results
RF_pred_train <- predict(RF_final, newdata = transformed_train)
RF_train_results <- confusionMatrix(transformed_train$V16, RF_pred_train)
print(RF_train_results)
# Test Set Results
RF_pred_test <- predict(RF_final, newdata = transformed_test)
RF_test_results <- confusionMatrix(transformed_test$V16, RF_pred_test)
print(RF_test_results)
set.seed(1)
#xg_spec <- boost_tree(
#    trees = 300,
#    tree_depth = tune(),
#    min_n = tune(),
#    mtry = tune(),
#    learn_rate = tune(),
#    loss_reduction = 0, # Not tuning this in this example
#    sample_size = 0.7  # Fixed in this example
#  ) %>%
#  set_mode("classification") %>%
#  set_engine("xgboost")
# Setup for cross-validation
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = transformed_train,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
## Plotting Results
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
# Prediction on Training Set
xg_pred_train <- predict(xg_fit, newdata = transformed_train)
confusionMatrix(xg_pred_train, transformed_train$V16)
# Prediction on Test Set
xg_pred_test <- predict(xg_fit, newdata = transformed_test)
confusionMatrix(xg_pred_test, transformed_train$V16)
set.seed(1)
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),
max_depth = c(3, 6, 9),  # Adjusted for simplification
eta = c(0.05, 0.1, 0.3),  # Learning rate
min_child_weight = c(1, 5, 10),
subsample = c(0.6, 0.8),  # Adjusted for simplification
gamma = 0,  # Minimum loss reduction required to make a further partition
colsample_bytree = 1)  # Subsample ratio of columns when constructing each tree
xg_fit <- train(V16 ~ .,
data = transformed_train,
method = "xgbTree",
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC")
## Plotting Results
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
# Prediction on Training Set
xg_pred_train <- predict(xg_fit, newdata = transformed_train)
confusionMatrix(transformed_train$V16,xg_pred_train)
# Prediction on Test Set
xg_pred_test <- predict(xg_fit, newdata = transformed_test)
confusionMatrix(transformed_train$V16,xg_pred_test)
## Plotting Results
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
# Prediction on Training Set
xg_pred_train <- predict(xg_fit, newdata = transformed_train)
confusionMatrix(transformed_train$V16,xg_pred_train)
# Prediction on Test Set
xg_pred_test <- predict(xg_fit, newdata = transformed_test)
confusionMatrix(transformed_train$V16,xg_pred_test)
## Plotting Results
trellis.par.set(caretTheme()) # optional
plot(xg_fit, metric = "ROC", plotType = "level")
# Prediction on Training Set
xg_pred_train <- predict(xg_fit, newdata = transformed_train)
confusionMatrix(transformed_train$V16,xg_pred_train)
# Prediction on Test Set
xg_pred_test <- predict(xg_fit, newdata = transformed_test)
confusionMatrix(transformed_test$V16,xg_pred_test)
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(recipes)
library(rpart)
library(rsample)
library(randomForest)
library(xgboost)
library(tidymodels)
library(tidyverse)
library(vip)
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(recipes)
library(rpart)
library(rsample)
library(randomForest)
library(xgboost)
library(tidymodels)
library(tidyverse)
library(vip)
library(nnet)
data <- read.table(file.choose(), header = FALSE, sep = ",")
str(data)
sapply(select(data,V1, V4, V5, V6, V7, V9, V10, V12, V13, V16), unique)
data$V2 <- as.numeric(data$V2)
data$V14 <- as.numeric(data$V14)
data <- data %>%
mutate(across(c(V1, V4, V5, V6, V7, V9, V10, V12, V13, V16), ~na_if(.x, "?")))
data$V16 <- factor(data$V16, levels = c("+", "-"),
labels = c("Positive", "Negative"))
blueprint <- recipe(V16 ~ ., data = data) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_impute_knn(all_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_dummy(all_nominal(), -all_outcomes())
blueprint_prep <- prep(blueprint, training = data)
set.seed(1)
index <- createDataPartition(data$V16, p = 0.7, list = FALSE)
data_train <- data[index, ]
data_test <- data[-index, ]
transformed_train <- bake(blueprint_prep, new_data = data_train)
transformed_test <- bake(blueprint_prep, new_data = data_test)
# Visualization
# Calculate the percentage
train_distribution <- data_train %>%
count(V16) %>%
mutate(Percentage = n / sum(n) * 100)
test_distribution <- data_test %>%
count(V16) %>%
mutate(Percentage = n / sum(n) * 100)
# Combine test and train set
combined_distribution <- bind_rows(
mutate(train_distribution, Set = "Train"),
mutate(test_distribution, Set = "Test")
)
# Plotting
ggplot(combined_distribution, aes(x = V16, y = Percentage, fill = Set)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "Target Feature", y = "Percentage", title = "Distribution of Target Feature in Training and Testing Sets") +
theme_minimal()
initial_rf <- randomForest(V16 ~ ., data = transformed_train)
plot(initial_rf)
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(mtry = c(13, 15, 17, 20, 22, 25),
splitrule = c("gini", "extratrees"),
min.node.size = c(5, 7, 9))
rf_fit <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
verbose = FALSE,
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC",
num.trees = 300)
## Plotting Results
ggplot(rf_fit)
ggplot(rf_fit, metric = "Sens")   # Sensitivity
#
trellis.par.set(caretTheme()) # optional
plot(rf_fit, metric = "ROC", plotType = "level")
# Final Model
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
RF_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(mtry = rf_fit$bestTune$mtry,
min.node.size = rf_fit$bestTune$min.node.size,
splitrule = rf_fit$bestTune$splitrule),
num.trees = 300)
# Training Set Results
RF_pred_train <- predict(RF_final, newdata = transformed_train)
RF_train_results <- confusionMatrix(transformed_train$V16, RF_pred_train)
print(RF_train_results)
# Test Set Results
RF_pred_test <- predict(RF_final, newdata = transformed_test)
RF_test_results <- confusionMatrix(transformed_test$V16, RF_pred_test)
print(RF_test_results)
RF_final <- train(V16 ~ .,
data = transformed_train,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
tuneGrid = data.frame(mtry = rf_fit$bestTune$mtry,
min.node.size = rf_fit$bestTune$min.node.size,
splitrule = rf_fit$bestTune$splitrule),
num.trees = 300)
# Training Set Results
RF_pred_train <- predict(RF_final, newdata = transformed_train)
RF_train_results <- confusionMatrix(transformed_train$V16, RF_pred_train)
print(RF_train_results)
# Test Set Results
RF_pred_test <- predict(RF_final, newdata = transformed_test)
RF_test_results <- confusionMatrix(transformed_test$V16, RF_pred_test)
print(RF_test_results)
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(recipes)
library(rpart)
library(rsample)
library(randomForest)
library(xgboost)
library(tidymodels)
library(tidyverse)
library(vip)
library(nnet)
data <- read.table(file.choose(), header = FALSE, sep = ",")
install.packages("data.table")
install.packages("data.table")
install.packages("data.table")
install.packages("data")
install.packages("data.table")
shiny::runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/first_app')
shiny::runApp('Desktop/STAT3106 AML/FinalAML/first_app')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
runApp('Desktop/STAT3106 AML/FinalAML/Credit_Card_Fraud_Detection')
